{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f5b702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\cbe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from efficientnet_pytorch) (2.2.2+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\cbe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\cbe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\cbe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\cbe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cbe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\cbe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->efficientnet_pytorch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cbe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\cbe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
      "Building wheels for collected packages: efficientnet_pytorch\n",
      "  Building wheel for efficientnet_pytorch (setup.py): started\n",
      "  Building wheel for efficientnet_pytorch (setup.py): finished with status 'done'\n",
      "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16464 sha256=fdb91429b1c7a46461ed44f07705c5fcd64bd9d198856302ec8c72bcd127d1f7\n",
      "  Stored in directory: c:\\users\\cbe\\appdata\\local\\pip\\cache\\wheels\\03\\3f\\e9\\911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
      "Successfully built efficientnet_pytorch\n",
      "Installing collected packages: efficientnet_pytorch\n",
      "Successfully installed efficientnet_pytorch-0.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a74ff8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to C:\\Users\\CBE/.cache\\torch\\hub\\checkpoints\\efficientnet-b7-dcc49843.pth\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 254M/254M [04:35<00:00, 968kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b7\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import tqdm\n",
    "# model = EfficientNet.from_name('efficientnet-b7') importing the Efficientnet model\n",
    "\n",
    "model = EfficientNet.from_pretrained('efficientnet-b7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89bf47ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2560,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "image_directory = './Before'\n",
    "\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_images(directory):\n",
    "    images = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.png') or filename.endswith('.jpg'):\n",
    "            img = cv2.imread(os.path.join(directory, filename))\n",
    "            if img is not None:\n",
    "                # Preprocess the image (resize)\n",
    "                img = cv2.resize(img, (128, 128))  # Resize to a standard size\n",
    "                # Convert to RGB if grayscale\n",
    "                if len(img.shape) == 2 or img.shape[2] == 1:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                images.append(img)\n",
    "    return images\n",
    "\n",
    "# Function to extract features from images\n",
    "def extract_features(images):\n",
    "    features = []\n",
    "    scaler = StandardScaler()\n",
    "    for img in images:\n",
    "        # Convert image to float32\n",
    "        img = img.astype(np.float32)\n",
    "        # Normalize the image (optional, depending on model requirements)\n",
    "        img = img / 255.0\n",
    "        # Add batch dimension\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img_tensor = torch.from_numpy(img).permute(0, 3, 1, 2)  # Change to [batch, channels, height, width]\n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            extracted_features = model.extract_features(img_tensor)\n",
    "        # Flatten the features into 2D array\n",
    "        extracted_features = extracted_features.squeeze().numpy()\n",
    "        num_channels, height, width = extracted_features.shape\n",
    "        flattened_features = extracted_features.reshape(num_channels, height * width).T  # Shape: [pixels, num_channels]\n",
    "        # Apply StandardScaler\n",
    "        normalized_feature = scaler.fit_transform(flattened_features)\n",
    "        features.append(normalized_feature[0])\n",
    "    return features\n",
    "\n",
    "# Load and preprocess images\n",
    "images = load_and_preprocess_images(image_directory)\n",
    "\n",
    "# Extract features from images\n",
    "Liste_of_features = extract_features(images)\n",
    "\n",
    "print(Liste_of_features[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fb31733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n",
      "318\n"
     ]
    }
   ],
   "source": [
    "print(len(Liste_of_features))\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1ba4a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 1 3 4 4 3 4 1 3 1 3 1 3 0 1 0 3 3 1 0 3 0 1 0 0 3 3 4 3 3 0 2 3 3 3\n",
      " 0 3 3 3 4 3 4 3 3 3 3 0 0 4 4 0 3 4 3 3 0 3 3 3 3 1 1 3 0 4 3 3 4 3 4 3 3\n",
      " 0 3 4 0 2 4 3 3 2 3 4 4 0 1 3 0 2 3 3 2 0 4 3 1 3 3 2 4 1 0 1 3 3 3 1 0 3\n",
      " 1 4 4 4 1 3 4 3 3 0 1 3 4 4 3 2 1 2 2 3 3 1 1 4 1 0 2 3 3 1 3 1 3 3 3 1 4\n",
      " 1 3 3 3 4 4 0 1 3 3 1 4 3 4 0 3 2 1 2 1 3 0 1 2 3 1 0 3 3 3 1 3 3 2 3 4 3\n",
      " 3 2 4 3 1 3 4 1 1 4 0 3 3 3 3 3 3 0 4 0 1 3 1 3 3 1 2 3 0 3 3 3 1 3 4 1 0\n",
      " 2 3 4 3 3 1 1 4 1 4 3 3 0 1 1 3 1 4 4 4 3 3 3 3 4 4 3 3 1 1 1 4 0 1 3 1 1\n",
      " 2 3 3 3 3 0 1 1 3 2 3 3 1 3 1 1 2 4 1 3 1 4 4 3 4 3 3 4 4 0 2 3 1 3 4 2 2\n",
      " 3 1 3 3 3 0 4 1 1 3 0 2 0 3 3 4 3 1 4 4 2 0]\n"
     ]
    }
   ],
   "source": [
    "all_features = Liste_of_features\n",
    "\n",
    "num_strata = 5\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=num_strata, random_state=0)\n",
    "strata_labels = kmeans.fit_predict(all_features)\n",
    "\n",
    "print(strata_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ca4956e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0, 2, 15, 17, 21, 23, 25, 26, 32, 37, 48, 49, 52, 57, 65, 74, 77, 86, 89, 94, 103, 109, 120, 136, 154, 162, 169, 174, 195, 202, 204, 213, 221, 234, 254, 264, 288, 301, 306, 308, 317], 1: [3, 9, 11, 13, 16, 20, 24, 62, 63, 87, 97, 102, 104, 108, 111, 115, 121, 127, 132, 133, 135, 140, 142, 146, 148, 155, 158, 165, 167, 170, 173, 178, 189, 192, 193, 205, 207, 210, 217, 220, 227, 228, 230, 235, 236, 238, 250, 251, 252, 255, 257, 258, 265, 266, 271, 273, 274, 277, 279, 291, 297, 303, 304, 313], 2: [1, 33, 78, 82, 90, 93, 100, 126, 128, 129, 137, 164, 166, 171, 181, 186, 211, 222, 259, 268, 275, 289, 294, 295, 307, 316], 3: [4, 7, 10, 12, 14, 18, 19, 22, 27, 28, 30, 31, 34, 35, 36, 38, 39, 40, 42, 44, 45, 46, 47, 53, 55, 56, 58, 59, 60, 61, 64, 67, 68, 70, 72, 73, 75, 80, 81, 83, 88, 91, 92, 96, 98, 99, 105, 106, 107, 110, 116, 118, 119, 122, 125, 130, 131, 138, 139, 141, 143, 144, 145, 149, 150, 151, 156, 157, 160, 163, 168, 172, 175, 176, 177, 179, 180, 182, 184, 185, 188, 190, 196, 197, 198, 199, 200, 201, 206, 208, 209, 212, 214, 215, 216, 218, 223, 225, 226, 232, 233, 237, 242, 243, 244, 245, 248, 249, 256, 260, 261, 262, 263, 267, 269, 270, 272, 278, 282, 284, 285, 290, 292, 296, 298, 299, 300, 305, 309, 310, 312], 4: [5, 6, 8, 29, 41, 43, 50, 51, 54, 66, 69, 71, 76, 79, 84, 85, 95, 101, 112, 113, 114, 117, 123, 124, 134, 147, 152, 153, 159, 161, 183, 187, 191, 194, 203, 219, 224, 229, 231, 239, 240, 241, 246, 247, 253, 276, 280, 281, 283, 286, 287, 293, 302, 311, 314, 315]}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store cells by strata\n",
    "strata_dict = {i: [] for i in range(num_strata)}\n",
    "\n",
    "for i, label in enumerate(strata_labels):\n",
    "    strata_dict[label].append(i)\n",
    "\n",
    "print(strata_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e40baa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sample size\n",
    "sample_size = 25  # Total number of images to sample\n",
    "sampled_images = []\n",
    "\n",
    "# Sample proportionally from each stratum\n",
    "for stratum, image_indices in strata_dict.items():\n",
    "    stratum_size = len(image_indices)\n",
    "    stratum_sample_size = int(sample_size * (stratum_size / len(images)))\n",
    "    \n",
    "    # Randomly sample images from the stratum\n",
    "    sampled_indices = np.random.choice(image_indices, stratum_sample_size, replace=False)\n",
    "    sampled_images.extend([images[i] for i in sampled_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9cd0f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = 'samples_output'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "for i, img in enumerate(sampled_images):\n",
    "    output_path = os.path.join(output_directory, f'sampled_image_{i}.png')\n",
    "    cv2.imwrite(output_path, img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
